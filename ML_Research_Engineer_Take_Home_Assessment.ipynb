{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vu93d2olS3m"
      },
      "source": [
        "# Design and Train an object detector to detect objects\n",
        "\n",
        "You have to design and implement a Training Pipeline that can train, test and visualize the model using the dataset provided.\n",
        "\n",
        "## Assignment Protocols\n",
        "\n",
        "- We expect it to take ~3 hours, with an extra 15 min for clear loom explanation(s)\n",
        "  - The assessment is timeboxed at 4 hours total in a single block. So please plan accordingly\n",
        "- You need to use Google Collaboratory to run and edit this notebook\n",
        "- You can only use Python as a programming Language\n",
        "- You need to use Pytorch, (and you cannot use PyTorch Lightning)\n",
        "- You cannot take help from any other person\n",
        "- You can use Google to search for references\n",
        "- You can not search on google for design-related things, like what should be loss function, or what should be model architecture.\n",
        "  - But you can use pre-trained backbones from PyTorch\n",
        "- Record a 5-10 mins of code walkthrough of the work you have done. You can use Loom Platform (https://www.loom.com) to record the video.\n",
        "  - Design Decisions\n",
        "    - Model Design which layers and activation functions you used and why\n",
        "    - Loss function, which loss functions you used and why\n",
        "    - What metrics in test function you would update and why?\n",
        "  - Any optimizations you have made to the codebase\n",
        "  - How you implemented resume functionality, what were the things you thought would be needed to resume training from exact same point\n",
        "  - Explain what parts of the assessment are completed and what is missing?\n",
        "  - Make sure to submit the screen recording link in the submission after you are done recording\n",
        "  - Please note that the free plan on Loom only allows for videos up to 5 minutes in length. As such, you may need to record two separate 5-minute videos.\n",
        "- [NO SUBMISSION WILL BE ACCEPTED WITHOUT]\n",
        "  - Trained best model weights\n",
        "  - Visualize Function in the Notebook\n",
        "  - Code Walk-through video\n",
        "\n",
        "## Task Details\n",
        "Design a Training Pipeline to train a object detector with following specs or assumptions:\n",
        "- Implement & Design Model\n",
        "  - You can use any backbone\n",
        "    - Either from PyTorch (torhvision) or any resource online\n",
        "    - But you need to design head your self (head means how you will use features of the back bone and get the desired outputs)\n",
        "  - Model needs to detect one object in each image\n",
        "  - Model should output following for each image passed as input:\n",
        "    - Whether we have an object or not\n",
        "    - Where is the object?\n",
        "      - The bounding box output format should be xmin, ymin, xmax, ymax\n",
        "      - It is not necessary the model is trained to output exactly this format but the visualize function which shows output should output in this format\n",
        "    - Either the object is a cat or dog?\n",
        "    - And which specie the object belongs to? There are in total 9 species:\n",
        "      - Cat [3 species]:\n",
        "        - Abyssinian\n",
        "        - Birman\n",
        "        - Persian\n",
        "      - Dog [6 species]\n",
        "        - american_bulldog\n",
        "        - american_pit_bull_terrier\n",
        "        - basset_hound\n",
        "        - beagle\n",
        "        - chihuahua\n",
        "        - pomeranian\n",
        "- Implement Custom Dataloader\n",
        "  - This is obvious as dataset is in a unique format any predifined dataloader wont work\n",
        "  - Follow best practices of writing custom dataloaders\n",
        "  - Details of the format of the dataset are defined in the Dataset Details section below\n",
        "  - Add needed pre-processing that you think would help train a better model or would help as we are using pre-trained weights as starting point\n",
        "  - Add augmentations that you think would help train a better model\n",
        "- Implement Loss Function\n",
        "  - Design and implement a loss function that can handle all of the outputs we have\n",
        "  - You can use pytorch built-in loss functions\n",
        "  - There are many scenarios which you need to handle, which one can understand from the dataset details and the model design\n",
        "- Update Resume Training Functionality using the best weights\n",
        "  - Current script does not have save best weights functionality\n",
        "  - The code should be able to resume training from exactly same point from where the training was stopped if model weights file is passed\n",
        "  - Keep in mind you can not resume training from same point by just loading weights of the model\n",
        "- Implement a visualize function [Most important, without this no submission will be accepted]\n",
        "  - The input of the function should be path of a folder with images and the weight file\n",
        "    - Also the output folder path to save outputs\n",
        "  - This function should return a dictionary of dictionaries with following details for each image:\n",
        "    - {\n",
        "        \"has_object\": True,\n",
        "        \"cat_or_dog\": \"cat\",\n",
        "        \"specie\": \"persian\",\n",
        "        \"xmin\": 10,\n",
        "        \"ymin\": 10,\n",
        "        \"xmax\": 10,\n",
        "        \"ymax\": 10\n",
        "    }\n",
        "  - And in case there is no object it should have 0 for bbox values, \"NA\" for \"cat_or_dog\" and \"specie\", and False for \"has_object\".\n",
        "  - Values of the returned dictionary should be like explained above and keys should be image names including the extension \".jpg\" or \".jpeg\"\n",
        "  - Should save output image with bounding box drawn on it, with same name input image but place in the output folder\n",
        "- Try to train the best model\n",
        "- Test function is already implemented but needs updates.\n",
        "  - [For classification outputs] Kindly choose best metrics (from the torch metrics library) according to the problem you are solving, and update the code to use that metrics\n",
        "    - You might also have to update the post-process function for heads according to model and loss function design\n",
        "  - [For Bounding Box output] If you have time you can update code to add metrics for BBOX output (this will be a plus), otherwise explain what metrics you would have used for BBOX and why?\n",
        "\n",
        "## Dataset Details\n",
        "The dataset has in total 1041 images. Each image has a single object which is either a cat or a dog.\n",
        "- There are multiple species for both cat and dog.\n",
        "- The number of images falling in each specie is as follows:\n",
        "  - basset_hound: 93\n",
        "  - Birman: 93\n",
        "  - pomeranian: 93\n",
        "  - american_pit_bull_terrier: 93\n",
        "  - american_bulldog: 93\n",
        "  - Abyssinian: 92\n",
        "  - beagle: 93\n",
        "  - Persian: 93\n",
        "  - chihuahua: 93\n",
        "  - empty: 142\n",
        "- The dataset has two folders:\n",
        "  - images\n",
        "    - Inside images folder we have 986 images in .jpg folder\n",
        "  - labels\n",
        "    - Inside labels folder we have 899 .xml files each file with details of image labels\n",
        "    - For any image that does not have a cat or dog, there is no corresponding xml file\n",
        "\n",
        "## Deliverable\n",
        "- Updated Colab Based Jupyter Notebook:\n",
        "  - With all the required functionality Implemented\n",
        "  - Which one can train the model without any errors\n",
        "  - One should achieve same metrics (Almost same metrics) if I run training using this collab notebook\n",
        "    - Set default values for everything accordingly in the notebook\n",
        "  - During evaluation we will just run the notebook and use the best weights the notebook saves automatically\n",
        "- Best weights you have trained\n",
        "  - We will Evaluate your weights against hold-out test we have and compare results\n",
        "  - We will use visualize function to generate outputs for each image\n",
        "  - Upload weights in an easily downloadable location like, Dropbox, Google Drive, Github, etc\n",
        "- A video code-walk through explaining your design decisions including but not limited to:\n",
        "  - Design Decisions\n",
        "    - Model Design which layers and activation functions you used and why\n",
        "    - Loss function, which loss functions you used and why\n",
        "    - What metrics in test function you would update and why?\n",
        "  - Any optimizations you have made to the codebase\n",
        "  - How you implemented resume functionality, what were the things you thought would be needed to resume training from exact same point\n",
        "  - Explain what parts of the assessment are completed and what is missing?\n",
        "\n",
        "## Evaluation Criteria\n",
        " - Design Decisions\n",
        " - Completeness: Did you include all features?\n",
        " - Correctness: Does the solution (all deliverables) work in sensible, thought-out ways?\n",
        " - Maintainability: Is the code written in a clean, maintainable way?\n",
        " - Testing: Is the solution adequately tested?\n",
        " - Documentation: Is the codebase well-documented and has proper steps to run any of the deliverables?\n",
        "\n",
        "## Extra Points\n",
        "- Any Updates in the notebook (Bugs/Implementation Mistakes etc)\n",
        "\n",
        "## How to submit\n",
        "- Please upload the Notebook for this project to GitHub, and post a link to your repository below [repo link box, on the left of submit button].\n",
        "  - Create a new GitHub repository from scratch\n",
        "  - Add the final Colab/Jupyter notebook to the repository\n",
        "- Please upload video and your final best weights on Google Drive or any other platform, and paste the link to the folder with both video and model in the text box just above the submit button.\n",
        "- Please paste the commit Id of the latest commit of your Github Repo, which should not be later than 4 hours of time when the repo was created.\n",
        "  - **Please note the submission without the commit id will not be considered.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MuJtyGcAskHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE_rGh4kod4n"
      },
      "source": [
        "# Install Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "magQ0ErkoOIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa263cde-3a40-4cfe-f30f-f71fc94a34cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.3)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.7.0->torchmetrics) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install bs4 lxml kaggle torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw3F4t1eaLl6"
      },
      "source": [
        "# Download Dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "59RSpyzRaOVj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'bilalyousaf0014'\n",
        "os.environ['KAGGLE_KEY'] = '11031bc21c5e3ec23585dbe17dc4267d'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "plZEhi_oaPPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9624607-bc01-48b8-966a-6781c111335b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ml-engineer-assessment-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d bilalyousaf0014/ml-engineer-assessment-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d6jkCK0uaxRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e044f77-d06e-4e4e-8d1e-59af8c9d4e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-engineer-assessment-dataset.zip\n",
            "replace assessment_dataset/images/00001.jpeg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "! unzip /content/ml-engineer-assessment-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMEbUqsPl9s4"
      },
      "source": [
        "# MODEL IMPLEMENTATION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "uXb6eKgFmNzx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "BtSgG4OklRte"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    ### Initialize the required Layers\n",
        "    resnet= torchvision.models.resnet34(pretrained=True)\n",
        "    layers=list(resnet.children())[:8]\n",
        "    self.features = nn.Sequential(*layers)\n",
        "    self.have_object = nn.Sequential(\n",
        "        nn.Linear(512,512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.Linear(512,2)\n",
        "        )\n",
        "    self.cat_or_dog = nn.Sequential(\n",
        "        nn.Linear(512,512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.Linear(512,2)\n",
        "        )\n",
        "    self.specie = nn.Sequential(\n",
        "        nn.Linear(512,512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.Linear(512,10)\n",
        "        )\n",
        "    self.bbox = nn.Sequential(\n",
        "        nn.Linear(512,512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.Linear(512,4)\n",
        "        )\n",
        "\n",
        "    self.pool=nn.AdaptiveAvgPool2d((1,1))\n",
        "    ### Initialize the required Layers\n",
        "\n",
        "  def forward(self, input):\n",
        "      ### Write Forward Calls for the Model\n",
        "      features = self.features(input)\n",
        "      features= torch.nn.functional.relu(features)\n",
        "\n",
        "      features=self.pool(features)\n",
        "      features=features.view(features.shape[0],-1)# flatten\n",
        "\n",
        "      return {\n",
        "          \"bbox\": self.bbox(features),\n",
        "          \"object\": self.have_object(features),\n",
        "          \"cat_or_dog\": self.cat_or_dog(features),\n",
        "          \"specie\": self.specie(features)\n",
        "      }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Loss_Function(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(Loss_Function, self).__init__()\n",
        "        self.classification = nn.CrossEntropyLoss()\n",
        "        self.bb = nn.L1Loss(reduction='mean')\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, pred, label, batch_size):\n",
        "        loss = 0\n",
        "        print(pred)\n",
        "        print(label)\n",
        "        for i in range(batch_size):\n",
        "            if label['specie'][i] == 0:\n",
        "                loss +=self.classification(pred['object'][i].to(self.device), torch.Tensor([0]).to(self.device))\n",
        "            else:\n",
        "\n",
        "                loss += self.bb(pred['bbox'][i].to(self.device), torch.Tensor([label['xmin'][i],label['ymin'][i],label['xmax'][i],label['ymax'][i]]).to(self.device))\n",
        "                loss += self.classification(pred['cat_or_dog'][i].to(self.device),label['cat_or_dog'][i].to(self.device))\n",
        "                loss += self.classification(pred['specie'][i].to(self.device),label['specie'][i].to(self.device))\n",
        "                loss +=self.classification(pred['object'][i].to(self.device), torch.Tensor([1]).to(self.device))\n",
        "\n",
        "        return loss/batch_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "\n",
        "input = torch.randn((2,3,224,224))\n",
        "output = model(input)"
      ],
      "metadata": {
        "id": "lbfzdjz1qZvC",
        "outputId": "7510d44e-7042-4fef-cae8-85a339f857e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avxCQ2W-oGxV"
      },
      "source": [
        "# CUSTOM DATALOADER IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "ZNds6zOg8w5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6315456-086d-4e26-f3f6-3f29aebf8bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chihuahua_100', 'american_pit_bull_terrier_157', 'chihuahua_186', 'Abyssinian_103', 'Abyssinian_115', 'american_bulldog_116', 'basset_hound_160', 'american_bulldog_123', 'chihuahua_126', 'basset_hound_161', 'basset_hound_168', 'american_bulldog_200', 'pomeranian_171', 'Birman_157', 'Birman_155', 'american_bulldog_101', 'american_pit_bull_terrier_135', 'Persian_186', 'Persian_125', 'pomeranian_133', 'beagle_164', 'beagle_138', 'american_pit_bull_terrier_179', 'Abyssinian_112', 'american_pit_bull_terrier_117', 'Abyssinian_13', 'Birman_116', 'american_bulldog_177', 'basset_hound_122', 'basset_hound_172', 'basset_hound_112', 'Abyssinian_174', 'american_pit_bull_terrier_171', 'beagle_151', 'pomeranian_118', 'basset_hound_182', 'american_pit_bull_terrier_102', 'pomeranian_132', 'american_bulldog_147', 'Birman_122', 'beagle_173', 'pomeranian_164', 'Persian_135', 'american_pit_bull_terrier_125', 'basset_hound_173', 'Birman_189', 'basset_hound_142', 'chihuahua_151', 'american_pit_bull_terrier_174', 'beagle_181', 'Birman_18', 'Persian_123', 'basset_hound_121', 'Abyssinian_141', 'Persian_17', 'basset_hound_176', 'beagle_168', 'Persian_172', 'Abyssinian_109', 'beagle_116', 'american_bulldog_106', 'basset_hound_18', 'pomeranian_112', 'beagle_113', 'beagle_128', 'pomeranian_148', 'pomeranian_150', 'basset_hound_188', 'american_pit_bull_terrier_158', 'american_pit_bull_terrier_184', 'Abyssinian_185', 'Abyssinian_106', 'american_bulldog_122', 'beagle_111', 'chihuahua_133', 'Persian_206', 'pomeranian_137', 'pomeranian_110', 'basset_hound_139', 'Persian_169', 'Abyssinian_11', 'Abyssinian_127', 'basset_hound_120', 'beagle_136', 'Abyssinian_157', 'american_pit_bull_terrier_153', 'beagle_123', 'Birman_159', 'chihuahua_146', 'Abyssinian_146', 'american_bulldog_192', 'beagle_131', 'pomeranian_169', 'Birman_186', 'american_bulldog_136', 'Persian_194', 'chihuahua_116', 'Persian_136', 'basset_hound_174', 'Birman_143', 'Persian_181', 'Abyssinian_165', 'pomeranian_182', 'Persian_202', 'american_pit_bull_terrier_148', 'pomeranian_139', 'american_pit_bull_terrier_177', 'beagle_152', 'basset_hound_17', 'american_pit_bull_terrier_130', 'basset_hound_125', 'basset_hound_131', 'beagle_127', 'Birman_169', 'beagle_107', 'beagle_154', 'Abyssinian_138', 'Abyssinian_130', 'pomeranian_184', 'pomeranian_160', 'Persian_116', 'Abyssinian_183', 'beagle_186', 'Persian_155', 'chihuahua_125', 'Abyssinian_192', 'pomeranian_183', 'Persian_129', 'Abyssinian_136', 'chihuahua_176', 'american_bulldog_100', 'Birman_120', 'Birman_136', 'basset_hound_171', 'chihuahua_162', 'chihuahua_179', 'pomeranian_102', 'Birman_153', 'Birman_171', 'Abyssinian_133', 'Persian_143', 'pomeranian_106', 'pomeranian_134', 'Birman_103', 'american_bulldog_113', 'chihuahua_114', 'american_pit_bull_terrier_144', 'chihuahua_110', 'pomeranian_175', 'beagle_167', 'Abyssinian_120', 'basset_hound_118', 'Abyssinian_182', 'Persian_106', 'american_pit_bull_terrier_149', 'beagle_100', 'chihuahua_187', 'basset_hound_147', 'basset_hound_190', 'Persian_160', 'american_bulldog_134', 'basset_hound_151', 'Abyssinian_117', 'chihuahua_147', 'basset_hound_181', 'american_pit_bull_terrier_128', 'Persian_180', 'american_pit_bull_terrier_127', 'american_bulldog_158', 'Birman_129', 'chihuahua_144', 'beagle_150', 'Persian_133', 'basset_hound_154', 'Abyssinian_137', 'basset_hound_133', 'american_bulldog_107', 'american_pit_bull_terrier_104', 'chihuahua_185', 'Abyssinian_158', 'american_pit_bull_terrier_151', 'beagle_110', 'pomeranian_119', 'chihuahua_135', 'chihuahua_109', 'beagle_137', 'pomeranian_136', 'Abyssinian_179', 'Birman_173', 'american_pit_bull_terrier_165', 'chihuahua_131', 'pomeranian_18', 'basset_hound_183', 'pomeranian_138', 'american_bulldog_196', 'american_bulldog_189', 'basset_hound_169', 'Birman_16', 'american_pit_bull_terrier_120', 'Persian_193', 'basset_hound_146', 'beagle_190', 'Birman_127', 'american_pit_bull_terrier_109', 'pomeranian_101', 'Abyssinian_100', 'Persian_101', 'chihuahua_143', 'american_bulldog_167', 'basset_hound_104', 'american_bulldog_179', 'beagle_101', 'american_pit_bull_terrier_103', 'pomeranian_146', 'chihuahua_112', 'pomeranian_162', 'beagle_160', 'basset_hound_132', 'chihuahua_188', 'beagle_135', 'pomeranian_142', 'Abyssinian_12', 'pomeranian_124', 'american_pit_bull_terrier_160', 'Persian_190', 'beagle_179', 'basset_hound_149', 'chihuahua_166', 'american_pit_bull_terrier_122', 'basset_hound_137', 'american_bulldog_201', 'american_bulldog_187', 'chihuahua_103', 'basset_hound_164', 'Persian_139', 'pomeranian_128', 'american_pit_bull_terrier_180', 'chihuahua_18', 'beagle_194', 'american_pit_bull_terrier_170', 'beagle_177', 'american_bulldog_175', 'american_pit_bull_terrier_118', 'american_bulldog_14', 'american_bulldog_139', 'chihuahua_152', 'american_pit_bull_terrier_131', 'pomeranian_186', 'american_bulldog_10', 'Persian_134', 'basset_hound_105', 'Persian_185', 'american_bulldog_164', 'american_pit_bull_terrier_143', 'american_bulldog_103', 'american_bulldog_117', 'chihuahua_148', 'pomeranian_189', 'pomeranian_143', 'pomeranian_161', 'pomeranian_131', 'Abyssinian_101', 'american_pit_bull_terrier_182', 'Birman_113', 'beagle_105', 'american_pit_bull_terrier_136', 'pomeranian_103', 'Persian_141', 'american_bulldog_202', 'basset_hound_165', 'Persian_118', 'pomeranian_147', 'american_pit_bull_terrier_183', 'Birman_100', 'Persian_173', 'basset_hound_109', 'Birman_104', 'Abyssinian_143', 'american_pit_bull_terrier_113', 'american_pit_bull_terrier_167', 'american_bulldog_181', 'Abyssinian_131', 'chihuahua_117', 'beagle_129', 'pomeranian_130', 'beagle_103', 'basset_hound_144', 'beagle_166', 'Abyssinian_148', 'american_pit_bull_terrier_145', 'Persian_147', 'american_bulldog_199', 'american_pit_bull_terrier_147', 'pomeranian_159', 'american_pit_bull_terrier_163', 'Persian_200', 'Birman_133', 'american_pit_bull_terrier_139', 'Abyssinian_153', 'chihuahua_141', 'beagle_102', 'pomeranian_190', 'beagle_147', 'beagle_170', 'Persian_149', 'chihuahua_160', 'american_pit_bull_terrier_162', 'Birman_158', 'Birman_140', 'basset_hound_136', 'chihuahua_120', 'basset_hound_159', 'american_bulldog_118', 'Abyssinian_152', 'american_pit_bull_terrier_178', 'chihuahua_180', 'Persian_18', 'Birman_174', 'Abyssinian_169', 'basset_hound_189', 'Abyssinian_113', 'Persian_19', 'basset_hound_103', 'american_bulldog_150', 'Abyssinian_180', 'american_bulldog_188', 'Abyssinian_166', 'Persian_162', 'beagle_185', 'chihuahua_140', 'american_bulldog_148', 'basset_hound_158', 'Birman_115', 'Persian_166', 'american_pit_bull_terrier_114', 'Birman_163', 'american_bulldog_104', 'Birman_164', 'american_pit_bull_terrier_100', 'basset_hound_153', 'Abyssinian_167', 'beagle_161', 'chihuahua_177', 'chihuahua_171', 'Abyssinian_135', 'Birman_102', 'Persian_164', 'beagle_175', 'pomeranian_158', 'chihuahua_102', 'american_bulldog_143', 'basset_hound_155', 'basset_hound_184', 'Persian_196', 'Birman_131', 'Abyssinian_121', 'pomeranian_120', 'american_pit_bull_terrier_119', 'Birman_162', 'Persian_182', 'Birman_150', 'Birman_109', 'chihuahua_157', 'american_pit_bull_terrier_155', 'american_pit_bull_terrier_189', 'pomeranian_121', 'basset_hound_163', 'american_bulldog_18', 'Birman_125', 'beagle_180', 'american_bulldog_185', 'Persian_117', 'Persian_126', 'chihuahua_138', 'Persian_179', 'Birman_180', 'Birman_111', 'Persian_132', 'Abyssinian_160', 'american_bulldog_191', 'american_bulldog_198', 'american_bulldog_133', 'chihuahua_118', 'pomeranian_109', 'chihuahua_129', 'Persian_131', 'basset_hound_111', 'basset_hound_115', 'beagle_124', 'Persian_16', 'basset_hound_145', 'Abyssinian_195', 'basset_hound_175', 'chihuahua_108', 'american_pit_bull_terrier_16', 'Birman_181', 'beagle_139', 'american_bulldog_121', 'Abyssinian_17', 'beagle_146', 'Birman_149', 'beagle_108', 'Birman_144', 'chihuahua_167', 'Persian_15', 'chihuahua_169', 'basset_hound_124', 'Abyssinian_19', 'Persian_159', 'Birman_178', 'pomeranian_149', 'american_bulldog_151', 'Persian_217', 'beagle_144', 'american_bulldog_183', 'Abyssinian_116', 'pomeranian_168', 'Abyssinian_128', 'Abyssinian_144', 'chihuahua_155', 'chihuahua_106', 'Persian_158', 'basset_hound_108', 'basset_hound_170', 'american_bulldog_171', 'Abyssinian_184', 'american_pit_bull_terrier_156', 'Birman_170', 'Persian_165', 'basset_hound_130', 'american_pit_bull_terrier_110', 'chihuahua_111', 'Persian_128', 'american_bulldog_152', 'Persian_183', 'Abyssinian_181', 'american_pit_bull_terrier_105', 'pomeranian_153', 'Persian_187', 'Birman_135', 'Abyssinian_170', 'Persian_152', 'beagle_156', 'beagle_163', 'american_bulldog_138', 'chihuahua_130', 'american_pit_bull_terrier_172', 'beagle_193', 'chihuahua_182', 'Birman_17', 'chihuahua_137', 'american_pit_bull_terrier_106', 'basset_hound_113', 'american_pit_bull_terrier_181', 'Persian_111', 'beagle_169', 'pomeranian_188', 'pomeranian_181', 'basset_hound_117', 'Abyssinian_156', 'Birman_168', 'Birman_156', 'american_bulldog_137', 'american_bulldog_178', 'Birman_184', 'Birman_101', 'american_pit_bull_terrier_107', 'american_bulldog_172', 'pomeranian_155', 'chihuahua_150', 'american_bulldog_119', 'Abyssinian_105', 'Abyssinian_176', 'american_bulldog_184', 'Abyssinian_178', 'beagle_130', 'basset_hound_100', 'chihuahua_121', 'chihuahua_158', 'Birman_142', 'Persian_161', 'Persian_107', 'beagle_178', 'beagle_145', 'beagle_141', 'beagle_174', 'american_bulldog_11', 'Persian_120', 'basset_hound_114', 'pomeranian_116', 'basset_hound_135', 'chihuahua_174', 'Birman_188', 'Birman_145', 'american_bulldog_193', 'pomeranian_129', 'american_bulldog_110', 'pomeranian_122', 'Persian_163', 'american_pit_bull_terrier_141', 'american_bulldog_132', 'american_pit_bull_terrier_152', 'beagle_126', 'beagle_172', 'american_pit_bull_terrier_116', 'basset_hound_127', 'american_pit_bull_terrier_146', 'Birman_160', 'pomeranian_167', 'american_bulldog_197', 'american_pit_bull_terrier_129', 'Birman_124', 'american_pit_bull_terrier_185', 'Birman_141', 'chihuahua_123', 'Abyssinian_172', 'beagle_189', 'chihuahua_105', 'chihuahua_124', 'american_bulldog_182', 'american_pit_bull_terrier_132', 'american_pit_bull_terrier_121', 'Persian_168', 'beagle_134', 'american_bulldog_149', 'pomeranian_16', 'Abyssinian_190', 'Abyssinian_126', 'basset_hound_128', 'chihuahua_183', 'american_pit_bull_terrier_168', 'chihuahua_127', 'pomeranian_172', 'american_pit_bull_terrier_126', 'american_pit_bull_terrier_187', 'Birman_126', 'chihuahua_16', 'american_bulldog_124', 'basset_hound_177', 'Persian_188', 'beagle_142', 'pomeranian_151', 'Birman_185', 'Birman_117', 'pomeranian_180', 'american_pit_bull_terrier_134', 'beagle_112', 'pomeranian_176', 'Abyssinian_132', 'basset_hound_162', 'american_bulldog_144', 'basset_hound_107', 'pomeranian_152', 'american_pit_bull_terrier_190', 'chihuahua_189', 'american_bulldog_105', 'Birman_167', 'beagle_17', 'chihuahua_132', 'Birman_107', 'Abyssinian_16', 'Persian_184', 'basset_hound_150', 'basset_hound_110', 'Birman_108', 'Abyssinian_122', 'Persian_191', 'pomeranian_165', 'Persian_175', 'Birman_154', 'chihuahua_149', 'american_pit_bull_terrier_188', 'Persian_189', 'basset_hound_141', 'Birman_119', 'american_pit_bull_terrier_166', 'Abyssinian_177', 'beagle_149', 'american_pit_bull_terrier_173', 'american_pit_bull_terrier_154', 'american_bulldog_153', 'Birman_161', 'pomeranian_140', 'beagle_182', 'Birman_134', 'pomeranian_107', 'chihuahua_172', 'Persian_137', 'Persian_140', 'Abyssinian_151', 'basset_hound_129', 'chihuahua_145', 'Persian_176', 'Abyssinian_15', 'american_bulldog_146', 'Birman_139', 'beagle_109', 'beagle_18', 'Persian_174', 'basset_hound_16', 'american_pit_bull_terrier_186', 'basset_hound_178', 'beagle_192', 'Abyssinian_191', 'american_bulldog_166', 'american_pit_bull_terrier_115', 'american_bulldog_180', 'chihuahua_128', 'Birman_182', 'american_pit_bull_terrier_138', 'Persian_156', 'pomeranian_123', 'Persian_108', 'pomeranian_141', 'Persian_115', 'american_pit_bull_terrier_161', 'Birman_190', 'Birman_172', 'basset_hound_101', 'american_pit_bull_terrier_176', 'beagle_122', 'beagle_148', 'Abyssinian_118', 'pomeranian_163', 'Abyssinian_123', 'pomeranian_115', 'Abyssinian_173', 'Persian_138', 'chihuahua_104', 'Birman_147', 'beagle_117', 'american_bulldog_109', 'Persian_153', 'chihuahua_173', 'Abyssinian_193', 'american_bulldog_130', 'chihuahua_190', 'Abyssinian_119', 'Birman_183', 'chihuahua_142', 'chihuahua_107', 'Birman_175', 'beagle_176', 'Persian_122', 'beagle_120', 'Persian_20', 'american_bulldog_127', 'chihuahua_170', 'pomeranian_17', 'chihuahua_119', 'pomeranian_125', 'american_bulldog_16', 'basset_hound_116', 'beagle_118', 'chihuahua_139', 'Birman_128', 'Abyssinian_102', 'american_bulldog_174', 'chihuahua_156', 'basset_hound_180', 'Abyssinian_161', 'Persian_144', 'basset_hound_185', 'chihuahua_164', 'Persian_105', 'american_bulldog_161', 'Birman_112', 'Abyssinian_10', 'Persian_170', 'Abyssinian_197', 'chihuahua_113', 'pomeranian_126', 'Abyssinian_14', 'beagle_187', 'american_bulldog_13', 'Persian_192', 'pomeranian_173', 'Abyssinian_155', 'Birman_151']\n"
          ]
        }
      ],
      "source": [
        "train_list = np.load('/content/assessment_dataset/train_list.npy', allow_pickle=True).tolist()\n",
        "val_list = np.load('/content/assessment_dataset/val_list.npy', allow_pickle=True).tolist()\n",
        "\n",
        "train_curated_list = []\n",
        "val_curated_list = []\n",
        "\n",
        "for i in train_list:\n",
        "    if os.path.exists(os.path.join(\"/content/assessment_dataset/images/\",f\"{i}.jpg\")):\n",
        "        train_curated_list.append(i)\n",
        "\n",
        "for i in val_list:\n",
        "    if os.path.exists(os.path.join(\"/content/assessment_dataset/images/\",f\"{i}.jpg\")):\n",
        "        val_curated_list.append(i)\n",
        "\n",
        "\n",
        "val_list = val_curated_list\n",
        "train_list = train_curated_list\n",
        "\n",
        "print(train_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "def create_mask(bb, x):\n",
        "    \"\"\"Creates a mask for the bounding box of same shape as image\"\"\"\n",
        "    rows,cols,*_ = x.shape\n",
        "    Y = np.zeros((rows, cols))\n",
        "    bb = bb.astype(np.int)\n",
        "    Y[bb[0]:bb[2], bb[1]:bb[3]] = 1.\n",
        "    return Y\n",
        "\n",
        "def mask_to_bb(Y):\n",
        "    \"\"\"Convert mask Y to a bounding box, assumes 0 as background nonzero object\"\"\"\n",
        "    cols, rows = np.nonzero(Y)\n",
        "    if len(cols)==0:\n",
        "        return np.zeros(4, dtype=np.float32)\n",
        "    top_row = np.min(rows)\n",
        "    left_col = np.min(cols)\n",
        "    bottom_row = np.max(rows)\n",
        "    right_col = np.max(cols)\n",
        "    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)\n",
        "\n",
        "def create_bb_array(x):\n",
        "    \"\"\"Generates bounding box array from a train_df row\"\"\"\n",
        "    return np.array([x[5],x[4],x[7],x[6]])\n",
        "\n",
        "\n",
        "def resize_image_bb(im,bb,sz):\n",
        "    \"\"\"Resize an image and its bounding box and write image to new path\"\"\"\n",
        "    # im = cv2.imread(read_path)\n",
        "    im_resized = cv2.resize(im, (sz,sz))\n",
        "    Y_resized = cv2.resize(create_mask(bb, im), (sz, sz))\n",
        "    # new_path = str(write_path/read_path.parts[-1])\n",
        "    #cv2.imwrite(new_path, cv2.cvtColor(im_resized, cv2.COLOR_RGB2BGR))\n",
        "    return im_resized, mask_to_bb(Y_resized)"
      ],
      "metadata": {
        "id": "edw_piZ7F8eK"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_list))"
      ],
      "metadata": {
        "id": "CmLe7L8i-SDY",
        "outputId": "94c010c6-8389-4573-a4f4-a799d069dd2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "E30zu6ICmKOL"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_xml_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        data = f.read()\n",
        "    bs_data = BeautifulSoup(data, 'xml')\n",
        "    return {\n",
        "        \"cat_or_dog\": bs_data.find(\"name\").text,\n",
        "        \"xmin\": int(bs_data.find(\"xmin\").text),\n",
        "        \"ymin\": int(bs_data.find(\"ymin\").text),\n",
        "        \"xmax\": int(bs_data.find(\"xmax\").text),\n",
        "        \"ymax\": int(bs_data.find(\"ymax\").text),\n",
        "        \"specie\": \"_\".join(path.split(os.sep)[-1].split(\"_\")[:-1])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "z16n7BbnoviB"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class CustomDataset():\n",
        "\n",
        "  def __init__(self, dataset_path, images_list, train=False):\n",
        "\n",
        "    #image_label_data = read_xml_file(\"/content/assessment_dataset/labels\")\n",
        "    self.dataset_path= dataset_path\n",
        "    self.images_list=images_list\n",
        "    self.cat_dog={\n",
        "\n",
        "                'cat':0,\n",
        "                'dog':1\n",
        "    }\n",
        "    self.species_list={\n",
        "        'basset_hound': 1,\n",
        "        'birman': 2,\n",
        "        'pomeranian': 3,\n",
        "        'american_pit_bull_terrier': 4,\n",
        "        'american_bulldog': 5,\n",
        "        'abyssinian': 6,\n",
        "        'beagle': 7,\n",
        "        'persian': 8,\n",
        "        'chihuahua': 9,\n",
        "        'empty': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images_list)\n",
        "  def __getitem__(self, index):\n",
        "    image = cv2.imread(os.path.join(self.dataset_path, f'images/{self.images_list[index]}.jpg'), cv2.COLOR_BGR2RGB)\n",
        "    # image=np.asarray(image)\n",
        "    # image=torch.from_numpy(image)\n",
        "    try:\n",
        "        label = read_xml_file(os.path.join(self.dataset_path,f'labels/{self.images_list[index]}.xml'))\n",
        "        image, labels = resize_image_bb(image,np.array([label['xmin'],label['ymin'],label['xmax'],label['ymax']]), 224)\n",
        "        label['specie']=torch.Tensor(self.species_list[label['specie'].lower()])\n",
        "        label['cat_or_dog']=torch.Tensor(self.cat_dog[label['cat_or_dog'].lower()])\n",
        "        label['xmin']=torch.Tensor([labels[0]])\n",
        "        label['ymin']=torch.Tensor([labels[1]])\n",
        "        label['xmax']=torch.Tensor([labels[2]])\n",
        "        label['ymax']=torch.Tensor([labels[3]])\n",
        "\n",
        "\n",
        "    except:\n",
        "        image, labels = resize_image_bb(image,[0,0,0,0], 224)\n",
        "        label={\n",
        "          'cat_or_dog':torch.Tensor(-1),\n",
        "          'xmin': torch.Tensor([-1]),\n",
        "          'ymin': torch.Tensor([-1]),\n",
        "          'xmax': torch.Tensor([-1]),\n",
        "          'ymax': torch.Tensor([-1]),\n",
        "          'specie':torch.Tensor(0)\n",
        "        }\n",
        "    # print(image.size)\n",
        "    # print(label)\n",
        "    image = torch.from_numpy(image).permute(2,0,1)\n",
        "    # image = torch.from_numpy(image)\n",
        "    return image,label\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=train_list)\n",
        "#training_loader = None"
      ],
      "metadata": {
        "id": "eeUeGhDpD5co"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b = training_dataset.__getitem__(100)\n",
        "print(a.shape)"
      ],
      "metadata": {
        "id": "RTwFWQsPEnR_",
        "outputId": "b88d9285-8dd1-4439-a29f-9f91256a40c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-210-f78f839443fb>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  bb = bb.astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkbn3C3cqahY"
      },
      "source": [
        "# TRAINING LOOP IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxODf-DLqmPA"
      },
      "source": [
        "## Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "Drkrrx8pqjzM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "1k8jvr7EqaPz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "################################## HELPER CODE PROVIDED BY HIRING TEAM ##################################\n",
        "\"\"\"\n",
        "This codebase is provided to help you finish the assessment in time.\n",
        "Yes, this code is not optimized, and properly formated. And there can be bugs, but best option\n",
        "for you is to use it as it is until you are done with all other aspects of the codebase. And then\n",
        "If you think that you are not able to achieve good results becuase this test function is problematic,\n",
        "then you can update it.\n",
        "\"\"\"\n",
        "import torchmetrics\n",
        "\n",
        "def test(model, val_loader):\n",
        "\n",
        "  def __tl(x):\n",
        "    return x.tolist()\n",
        "\n",
        "  def __tn(x):\n",
        "    return x.detach().cpu().numpy()\n",
        "\n",
        "  def __tnl(x):\n",
        "    return (x.detach().cpu().numpy()).tolist()\n",
        "\n",
        "  def post_process_object(x):\n",
        "    return torch.where(x > 0.5, 1.0, 0.0).squeeze(1)\n",
        "\n",
        "  def post_process_cat_or_dog(x):\n",
        "    return torch.where(x > 0.5, 1.0, 0.0).squeeze(1)\n",
        "\n",
        "  def post_process_specie(x):\n",
        "    return torch.argmax(x, dim=1)\n",
        "\n",
        "  def post_process_xmin(x):\n",
        "    return x\n",
        "\n",
        "  def post_process_ymin(x):\n",
        "    return x\n",
        "\n",
        "  def post_process_xmax(x):\n",
        "    return x\n",
        "\n",
        "  def post_process_ymax(x):\n",
        "    return x\n",
        "\n",
        "  metric_object = torchmetrics.Accuracy(task=\"binary\")\n",
        "  metric_cat_or_dog = torchmetrics.Accuracy(task=\"binary\")\n",
        "  metric_specie = torchmetrics.Accuracy(task=\"multiclass\", num_classes=9)\n",
        "\n",
        "  output_list = {\n",
        "      \"object\": [],\n",
        "      \"cat_or_dog\": [],\n",
        "      \"specie\": [],\n",
        "      \"xmin\": [],\n",
        "      \"ymin\": [],\n",
        "      \"xmax\": [],\n",
        "      \"ymax\": [],\n",
        "  }\n",
        "  labels_list = {\n",
        "      \"object\": [],\n",
        "      \"cat_or_dog\": [],\n",
        "      \"specie\": [],\n",
        "      \"xmin\": [],\n",
        "      \"ymin\": [],\n",
        "      \"xmax\": [],\n",
        "      \"ymax\": [],\n",
        "  }\n",
        "\n",
        "  for i, data in enumerate(val_loader):\n",
        "    inputs, labels = data\n",
        "    if torch.cuda.is_available():\n",
        "      inputs = inputs.cuda()\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    is_object = __tnl(labels[\"have_object\"])\n",
        "    width = __tn(labels[\"width\"])\n",
        "    height = __tn(labels[\"height\"])\n",
        "    output_list[\"object\"].extend(__tnl(post_process_object(outputs[\"object\"])))\n",
        "    labels_list[\"object\"].extend(__tnl(labels[\"have_object\"]))\n",
        "\n",
        "    if is_object[0] == 1.0:\n",
        "      output_list[\"cat_or_dog\"].extend(\n",
        "        __tnl(post_process_cat_or_dog(outputs[\"cat_or_dog\"]))\n",
        "      )\n",
        "      labels_list[\"cat_or_dog\"].extend(\n",
        "        __tnl(labels[\"cat_or_dog\"])\n",
        "      )\n",
        "      output_list[\"specie\"].extend(\n",
        "        __tnl(post_process_specie(outputs[\"specie\"]))\n",
        "      )\n",
        "      labels_list[\"specie\"].extend(__tnl(labels[\"specie\"]))\n",
        "      output_list[\"xmin\"].extend(\n",
        "        __tl(__tn(post_process_xmin(outputs[\"bbox\"][:, 0]))*width)\n",
        "      )\n",
        "      labels_list[\"xmin\"].extend(__tl(__tn(labels[\"xmin\"])*width))\n",
        "      output_list[\"ymin\"].extend(\n",
        "          __tl(__tn(post_process_ymin(outputs[\"bbox\"][:, 1]))*height)\n",
        "      )\n",
        "      labels_list[\"ymin\"].extend(\n",
        "          __tl(__tn(labels[\"ymin\"])*height)\n",
        "      )\n",
        "      output_list[\"xmax\"].extend(\n",
        "          __tl(__tn(post_process_xmax(outputs[\"bbox\"][:, 2]))*width)\n",
        "      )\n",
        "      labels_list[\"xmax\"].extend(__tl(__tn(labels[\"xmax\"])*width))\n",
        "      output_list[\"ymax\"].extend(__tl(__tn(post_process_ymax(outputs[\"bbox\"][:, 3]))*height))\n",
        "      labels_list[\"ymax\"].extend(__tl(__tn(labels[\"ymax\"])*height))\n",
        "\n",
        "  score_object = metric_object(torch.tensor(output_list[\"object\"]), torch.tensor(labels_list[\"object\"]))\n",
        "  score_cat_or_dog = metric_cat_or_dog(torch.tensor(output_list[\"cat_or_dog\"]), torch.tensor(labels_list[\"cat_or_dog\"]))\n",
        "  score_specie = metric_specie(torch.tensor(output_list[\"specie\"]), torch.tensor(labels_list[\"specie\"]))\n",
        "  score_bbox = None\n",
        "  return score_object, score_cat_or_dog, score_specie, score_bbox\n",
        "################################## HELPER CODE PROVIDED BY HIRING TEAM ##################################\n",
        "\n",
        "\n",
        "def train(epochs, model_weights):\n",
        "\n",
        "  # Initialize Model and Optimizer\n",
        "  if torch.cuda.is_available():\n",
        "     device='cuda'\n",
        "  else:\n",
        "     device='cpu'\n",
        "\n",
        "  model = Model().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
        "  # Initialize Loss Functions\n",
        "  # have_object_loss = None\n",
        "  # specie_loss = None\n",
        "  # cat_or_dog_loss = None\n",
        "  # bbox_loss = None # Not necessary you need to apply function to all coordinates together, You can have separete loss functions for all coordinates too\n",
        "  # # Below or Above\n",
        "  # xmin_loss = None\n",
        "  # ymin_loss = None\n",
        "  # xmax_loss = None\n",
        "  # ymax_loss = None\n",
        "  Lossfunction=Loss_Function(device = device)\n",
        "  training_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=train_list)\n",
        "  training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=16, num_workers=0, shuffle=True)\n",
        "\n",
        "  def train_one_epoch(epoch_index, tb_writer):\n",
        "      running_loss = 0.\n",
        "      last_loss = 0.\n",
        "\n",
        "      # Here, we use enumerate(training_loader) instead of\n",
        "      # iter(training_loader) so that we can track the batch\n",
        "      # index and do some intra-epoch reporting\n",
        "      for i, data in enumerate(training_loader):\n",
        "          # Every data instance is an input + label pair\n",
        "          inputs, labels = data\n",
        "\n",
        "          inputs = inputs.float().to(device)\n",
        "          # labels = labels.to(device)\n",
        "\n",
        "          # Make predictions for this batch\n",
        "          outputs = model(inputs)\n",
        "          loss=Lossfunction(outputs, labels, batch_size=inputs.shape[0])\n",
        "\n",
        "          # Compute the loss and its gradients\n",
        "          # loss_have_object = have_object_loss(outputs[\"object\"], None)\n",
        "          # loss_specie = specie_loss(outputs[\"specie\"], None)\n",
        "          # loss_cat_or_dog = cat_or_dog_loss(outputs[\"cat_or_dog\"], None)\n",
        "\n",
        "          # loss_bbox = bbox_loss(outputs[\"bbox\"], None)\n",
        "          # # Above or Below\n",
        "          # loss_xmin = xmin_loss(outputs[\"bbox\"], None)\n",
        "          # loss_ymin = ymin_loss(outputs[\"bbox\"], None)\n",
        "          # loss_xmax = xmax_loss(outputs[\"bbox\"], None)\n",
        "          # loss_ymax = ymax_loss(outputs[\"bbox\"], None)\n",
        "\n",
        "          # loss =  # Consolidate all individual losses\n",
        "\n",
        "          # Gather data and report\n",
        "          running_loss += loss.item()\n",
        "          if i % 10 == 0:\n",
        "              last_loss = running_loss / 10 # loss per batch\n",
        "              running_loss = 0.\n",
        "      return last_loss\n",
        "\n",
        "  for i in range(2):\n",
        "\n",
        "    epoch_loss = train_one_epoch(i, None)\n",
        "    print(f' Epoch {i} Loss : {epoch_loss}')\n",
        "\n",
        "    torch.save(\"model.pth\", model.state_dict())\n",
        "    #metrics = test(model)\n",
        "    #print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "8TgRVGPDaUAd"
      },
      "outputs": [],
      "source": [
        "\n",
        "from PIL import ImageDraw\n",
        "\n",
        "def visualize(model_weights, image_folder_path, output_folder=\"output\"):\n",
        "\n",
        "  model = Model()\n",
        "  model.load_state_dict(torch.load(model_weights))\n",
        "\n",
        "  try:\n",
        "    image = Image.open(os.path.join(\"/content/assessment_dataset/images\", image_name+\".jpg\"))\n",
        "  except:\n",
        "    image = Image.open(os.path.join(\"/content/assessment_dataset/images\", image_name+\".jpeg\"))\n",
        "\n",
        "  preprocess = None\n",
        "  output = model()\n",
        "  return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "u7npVwns6LU6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b44664a5-5119-4c67-fb5e-23c9b74c6d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-210-f78f839443fb>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  bb = bb.astype(np.int)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-225-36b74cbdf5be>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-217-6912e515ccb6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model_weights)\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' Epoch {i} Loss : {epoch_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-217-6912e515ccb6>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m    145\u001b[0m       \u001b[0;31m# iter(training_loader) so that we can track the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0;31m# index and do some intra-epoch reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m           \u001b[0;31m# Every data instance is an input + label pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m           \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# The mapping type may not support `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# The mapping type may not support `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [0] at entry 0 and [1] at entry 1"
          ]
        }
      ],
      "source": [
        "train(2,None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6XqwIY8oM3q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}